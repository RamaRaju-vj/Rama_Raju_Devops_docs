{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Note</p> <p>This documentation is work in progress!!!</p> <p>Tip</p>"},{"location":"#recommended-chrome-extensions-for-summarizing","title":"\ud83d\udd0d Recommended Chrome Extensions for Summarizing","text":"<p>Please use the following Chrome extensions if you want a summarized version of each topic:</p> <ul> <li>Condense \u2013 Summarize &amp; Chat  (Not sponsored)</li> <li>Storytell \u2013 AI Summarizer (Not sponsored)</li> </ul> <p>Note: First navigate to the topic you need, and then use the extension for the summary.</p>"},{"location":"#folder-descriptions","title":"\ud83d\udccc Folder Descriptions","text":"<pre><code>\ud83d\udcc2 docs/\n\u2502  \n\u251c\u2500\u2500 \ud83d\ude80  Ram Portfolio          \u2192 Personal Web Portfolio\n\u2502  \n\u251c\u2500\u2500 \u2638\ufe0f  Kubernetes/            \u2192 Kubernetes configurations, deployments, and best practices.\n\u2502  \n\u251c\u2500\u2500 \u2699\ufe0f  CI-CD                  \u2192 Continuous Integration and Deployment practices.\n\u2502  \n\u251c\u2500\u2500 \ud83c\udf10  Networking             \u2192 Network protocols, cloud networking, and security.\n\u2502  \n\u251c\u2500\u2500 \ud83c\udfd7\ufe0f  Terraform              \u2192 Infrastructure as Code (IaC) using Terraform.\n\u2502  \n\u251c\u2500\u2500 \ud83d\udc0d  Python+Scripting       \u2192 Python automation, scripting, and DevOps tools.\n\u2502  \n\u251c\u2500\u2500 \ud83d\udc27  Linux+Scripting        \u2192 Linux commands, Bash scripting, and automation.\n\u2502  \n\u251c\u2500\u2500 \ud83d\udc33  Docker                 \u2192 Application Containerization\n\u2502\n\u251c\u2500\u2500 \ud83d\udcca  Monitoring &amp; Logging   \u2192 Logging, Prometheus, Grafana, and observability tools.\n\u2502  \n\u2514\u2500\u2500 \ud83d\udd00  Git                    \u2192 Version control, branching strategies, and workflows.\n</code></pre>"},{"location":"%F0%9F%9A%80%20Ram-Portfolio/","title":"\ud83d\ude80 Ram Portfolio","text":""},{"location":"%F0%9F%9A%80%20Ram-Portfolio/#ram-portfolio","title":"\ud83d\ude80 Ram Portfolio","text":""},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/","title":"Index","text":""},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/#k8s-index","title":"K8s Index","text":"<p>Use the links \ud83d\udc47 to navigate through topics if needed.</p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/#core-concepts","title":"\ud83d\udcd8 Core Concepts","text":"<ul> <li>1.1 Kubernetes Architecture</li> <li>1.2 Kubernetes Components</li> <li>1.3 Kubernetes Services</li> <li>1.4 Kubernetes Namespaces</li> <li>1.5 Imperative and Declarative Approaches</li> <li>1.6 Practice Resources</li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.1%20k8s-architecture/","title":"1.1 k8s architecture","text":""},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.1%20k8s-architecture/#official-kubernetes-documentation","title":"Official Kubernetes documentation","text":"<p>Note</p> <p>The <code>crictl</code> is from the Kubernetes community and works across all CRI compatible runtimes and are used mainly for debugging purposes.</p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.1%20k8s-architecture/#available-cris-containerd-rkt","title":"Available CRI's - <code>ContainerD</code>, <code>\ud83d\ude80rkt</code>","text":""},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.1%20k8s-architecture/#watch-the-video","title":"Watch the Video","text":""},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.2%20k8s-components/","title":"1.2 k8s components","text":""},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.2%20k8s-components/#kubernetes-components","title":"Kubernetes Components","text":"1. ETCD (Click to Expand) 2.Kube-Api Server  (Click to Expand) 3.Kube Controller Manager  (Click to Expand) 4.kube-scheduler (Click to Expand) 5.kubelet (Click to Expand) 6.kube-proxy (Click to Expand) 7.Pods (Click to Expand) 8. ReplicaSets (Click to Expand) 9. Deployment (Click to Expand)"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.2%20k8s-components/#etcd","title":"ETCD","text":"<ul> <li> <p>The etcd key value data store stores information regarding the cluster such as the nodes, pods, convicts, secrets, accounts, roles,role bindings, and others. Every information you see when you run the kube control get command is from the etcd server.</p> </li> <li> <p>Every change you make to your cluster such as adding additional nodes, deploying pods or replica sets are updated in the etcd server. Only once it is updated in the etcd server is the change considered to be complete. </p> </li> <li> <p>ETCD V3 Commands </p> </li> <li> <p>If you set up your cluster using kubeadm, then kubeadm deploys the etcd server for you as a pod in the kube system namespace. </p> </li> <li> <p>In a high availability(HA) environment, you will have multiple master nodes in your cluster. Then you will have multiple etcd instances spread across the master nodes. In that case, make sure that the etcd instances know about each other by setting the right parameter in the etcd service configuration.</p> </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.2%20k8s-components/#kube-api-server","title":"Kube-Api Server","text":"<ul> <li> <p>Let's look at an example of creating a pod. When you do that, as before,the request is authenticated first and then validated. In this case, the API server creates a pod object without assigning it to a node.Updates the information in the etcd server,updates the user that the pod has been created.</p> </li> <li> <p>The scheduler continuously monitors the API server and realizes that there is a new pod with no node assigned.The scheduler identifies the right worker node to place the new pod on. The API server then updates the informationin the etcd cluster.The API server then passes that information to the kubelet in the appropriate worker node.The kubelet then creates the podon the node and instructs the container runtime engineto deploy the application image.Once done, the kubelet updates the statusback to the API serverand the API server then updates the databack in the etcd cluster. A similar pattern is followed every time a change is requested. The kube-apiserver is at the center of all the different tasks that needs to be performed to make a change in the cluster.</p> </li> <li> <p>To summarize, the kube-apiserver is responsible for authenticating and validating requests, retrieving and updating data in the etcd data store. In fact, kube-apiserver is the only component that interacts directly with the etcd data store. The other components, such as the scheduler, kube-controller-manager and kubelet uses the API server to perform updates in the cluster in their respective areas.</p> </li> </ul> <p></p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.2%20k8s-components/#kube-controller-manager","title":"Kube Controller Manager","text":"<ul> <li> <p>A controller is a process that continuously monitors the state of various components within the system and works towards bringing the whole system to the desired functioning state.</p> </li> <li> <p>For example, the node controller is responsible for monitoring the status of the nodes and taking necessary actions to keep the applications running. It does that through the Kube API server. The node controller tests the status of the nodes every five seconds. That way the node controller can monitor the health of the nodes. If it stops receiving heartbeat from a node the node is marked as unreachable but it waits for 40 seconds before marking it unreachable. After a node is marked unreachable it gives it 5 minutes to come back up. If it doesn't, it removes the PODs assigned to that node and provisions them on the healthy ones. </p> </li> <li> <p>if the PODs are part of a replica set. The next controller is the replication controller. It is responsible for monitoring the status of replica sets and ensuring that the desired number of PODs are available at all times within the set. If a POD dies, it creates another one. Now, those were just two examples of controllers. There are many more such controllers available within Kubernetes. </p> </li> <li> <p>Now, how do you see these controllers and where are they located in your cluster? They're all packaged into a single process known as the Kubernetes Controller Manager. When you install the Kubernetes controller manager the different controllers get installed as well.</p> </li> </ul> <p></p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.2%20k8s-components/#kube-scheduler","title":"kube-scheduler","text":"<ul> <li> <p>The scheduler looks at each pod and tries to find the best node for it.For example, let's take one of these pods, the big blue one.It has a set of CPU and memory requirements.The scheduler goes through two phasesto identify the best node for the pod.In the first phase, the scheduler tries to filter outthe nodes that do not fit the profile for this pod.For example, the nodes that do not have sufficient CPU and memory resources requested by the pod.So the first two small nodes are filtered out.So we are now left with the two nodeson which the pod can be placed.Now how does the scheduler pick one from the two?The scheduler ranks the nodesto identify the best fit for the pod.It uses a priority function to assign a scoreto the nodes on a scale of zero to 10.For example, the scheduler calculatesthe amount of resources that would be freeon the nodes after placing the pod on them.In this case, the one on the rightwould have six CPUs free if the pod was placed on it,which is four more than the other one.So it gets a better rank, and so it wins. </p> </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.2%20k8s-components/#note-remember-the-scheduler-is-only-responsiblefor-deciding-which-pod-goes-on-which-nodeit-doesnt-actually-place-the-pod-on-the-nodesthats-the-job-of-the-kubelet","title":"Note: Remember, the scheduler is only responsiblefor deciding which pod goes on which node.It doesn't actually place the pod on the nodes.That's the job of the kubelet.","text":""},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.2%20k8s-components/#kubelet","title":"kubelet","text":"<ul> <li> <p>The kubelet in the Kubernetes worker node registers the node with a Kubernetes cluster. When it receives instructions to load container or a pod on the node,it requests the container runtime engine,which may be Docker, to pull the required image and run an instance.The kubelet then continues to monitor the state of the pod and containers in it and reports to the kube API serveron a timely basis.</p> </li> <li> </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.2%20k8s-components/#note-so-how-do-you-install-the-kubelet-if-you-use-the-kubeadm-tool-to-deploy-your-clusterit-does-not-automatically-deploy-the-kubelet-now-thats-the-difference-from-other-componentsyou-must-always-manually-install-the-kubelet-on-your-worker-nodes","title":"Note: So how do you install the kubelet? If you use the kubeadm tool to deploy your cluster,it does not automatically deploy the kubelet. Now that's the difference from other components.You must always manually install the kubelet on your worker nodes.","text":""},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.2%20k8s-components/#kube-proxy-networking-and-communication","title":"kube-proxy (networking and Communication)","text":"<ul> <li> <p>Kube-proxy is a process that runs on each node in the Kubernetes cluster. Its job is to look for new services, and every time a new service is created, it creates the appropriate rules on each node to forward traffic to those services to the backend pods. One way it does this is using iptables rules. In this case, it creates an iptables rule on each node in the cluster to forward traffic heading to the IP of the service, which is 10.96.0.12, to the IP of the actual pod, which is 10.32.0.15. So that's how kube-proxy configures a service. </p> </li> <li> <p>The kubeadm tool deploys kube-proxy as pods on each node. In fact, it is deployed as a DaemonSet, so a single pod is always deployed on each node in the cluster. </p> </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.2%20k8s-components/#pods","title":"Pods","text":"<ul> <li> <p>A pod is a single instance of an application. A pod is the smallest object that you can create in Kubernetes. Here we see the simplest of simplest cases where you have a single-node Kubernetes cluster with a single instance of your application running in a single Docker container encapsulated in a pod.</p> </li> <li> <p>What if the number of users accessing your application increase and you need to scale your application? We create a new pod altogether with a new instance of the same application. As you can see, we now have two instances of our web application running on two separate pods on the same Kubernetes system or node.</p> </li> </ul> <p></p> <ul> <li> <p>What if the user base further increases and your current node has no sufficient capacity? Well, then you can always deploy additional pods on a new node in the cluster.</p> </li> <li> <p>You will have a new node added to the cluster to expand the cluster's physical capacity. Pods usually have a one-to-one relationship with containers running your application. To scale up, you create new pods, and to scale down, you delete existing pods. You do not add additional containers to an existing pod to scale your application.</p> </li> </ul> <p></p> <ul> <li>So in the current state, we haven't made the web server accessible to external users. You can, however, access it internally from the node. Once we learn about networking and services, we will get to know how to make this service accessible to end users.</li> </ul> Pod Configuration in YAML (Click to Expand) <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app: myapp            # Label 1\n    type: backend-process # Label 2\nspec:\n  containers:\n    - name: myapp-container\n      image: &lt;Image&gt;\n      resources:\n        limits:\n          memory: \"128Mi\"\n          cpu: \"500m\"\n      ports:\n        - containerPort: 8080\n</code></pre> <p> </p> <ul> <li>Containers is a list or an array. The reason this property is a list is because the pods can have multiple containers within them</li> </ul> Commands to Create Pod and describe pod (Click to Expand) pod-definition.yaml<pre><code>kubectl create -f pod-definition.yaml\n</code></pre> To Get Pods<pre><code>kubectl get pods\n</code></pre> To Describe Pods<pre><code>kubectl describe pod myapp-pod\n</code></pre>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.2%20k8s-components/#replicasets","title":"ReplicaSets","text":"<ul> <li> <p>To prevent users from losing access to our application, we would like to have more than one instance or pod running at the same time. That way, if one fails we still have our application running on the other one. The Replication Controller helps us run multiple instances of a single pod in the Kubernetes cluster.</p> </li> <li> <p>Even if you have a single pod, the Replication Controller can help by automatically bringing up a new pod when the existing one fails. Thus, the Replication Controller ensures that the specified number of pods are running at all times even if it's just one or 100. Another reason we need Replication Controller is to create multiple pods to share the load across them. </p> </li> <li> <p>If the demand further increases and if we were to run out of resources on the first node we could deploy additional parts across the other nodes in the cluster. As you can see, the Replication Controller spans across multiple nodes in the cluster. It helps us balance the load across multiple pods on different nodes as well as scale our application when the demand increases.</p> </li> </ul> ReplicaSet Configuration in YAML (Click to Expand) <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: myreplicaset\n  labels:\n    app: myapp\n    key: value\nspec:\n  replicas: &lt;Replicas&gt;\n  selector:\n    matchLabels:\n      key: value\n  template:\n    metadata:\n      labels:\n        key: value\n    spec:\n      containers:\n        - name: myapp\n          image: &lt;Image&gt;\n</code></pre> <ul> <li> <p>Replica Set can also manage pods that were not created as part of the Replica Set creation. Say for example, there were pods created before the creation of the Replica Set that match labels specified in the selector, the Replica Set will also take those pods into consideration when creating the replicas.</p> </li> <li> <p>The role of the Replica Set is to monitor the pods and if any of them were to fail, deploy new ones. The Replica Set is in fact a process that monitors the pods. Now, how does the Replica Set know what pods to monitor? There could be hundreds of other pods in the cluster running different applications. This is where labeling our pods during creation comes in handy. We could now provide these labels as a filter for Replica Set. Under the selector section, we use the match labels filter and provide the same label that we used while creating the pods.This way, the Replica Set knows which pods to monitor. The same concept of labels and selectors is used in many other places throughout Kubernetes.</p> </li> </ul> ReplicaSet Commands replicaset-definition.yaml<pre><code>    kubectl create -f replicaset-definition.yaml\n</code></pre> To Get replicaset<pre><code>    kubectl get replicaset\n</code></pre> To Describe Pods<pre><code>    kubectl describe pod myapp-pod\n</code></pre> To Replace ReplicaSet Definition<pre><code>    kubectl replace -f replicaset-definition.yml\n</code></pre> To Scale ReplicaSet using file<pre><code>    kubectl scale --replicas=6 -f replicaset-definition.yml\n</code></pre> To Scale ReplicaSet using name<pre><code>    kubectl scale --replicas=6 replicaset myapp-replicaset\n</code></pre> Delete ReplicaSet (also deletes all underlying pods)<pre><code>    kubectl delete replicaset replicaset-name\n</code></pre>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.2%20k8s-components/#deployment","title":"Deployment","text":"Deployment Configuration in YAML (Click to Expand) <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n        - name: myapp\n          image: &lt;Image&gt;\n          resources:\n            limits:\n              memory: \"128Mi\"\n              cpu: \"500m\"\n          ports:\n            - containerPort: &lt;Port&gt;\n</code></pre> Deployment Commands Deployment-definition.yaml<pre><code>    kubectl create -f Deployment-definition.yaml\n</code></pre> To see all the created objects at once<pre><code>    kubectl get all\n</code></pre>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.3%20K8s-services/","title":"1.3 K8s services","text":"<p>Kubernetes Services facilitate communication both within the application and with external users. They connect different groups of Pods\u2014such as frontend, backend, and those interacting with external data sources\u2014allowing them to work together seamlessly. Services ensure the frontend is accessible to users, enable backend-frontend interaction, and support connections to external systems. This allows for loose coupling between microservices, making the application more modular and scalable.</p> <p></p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.3%20K8s-services/#kubernetes-offers-three-main-types-of-services-for-exposing-applications","title":"Kubernetes offers three main types of Services for exposing applications","text":""},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.3%20K8s-services/#node-port-service","title":"Node Port Service","text":"<p>Exposes a Pod on a specific port of the node, making it accessible externally.   The Kubernetes Service is an object, just like Pods, ReplicaSets, or Deployments. One of its use cases is to listen to a port on the node and forward requests on that port to a port on the Pod running the web application.   This type of Service is known as a NodePort Service because it listens on a port on the node and forwards requests to the Pods.</p> <p> </p> Node Port Configuration (Click to Expand) <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: myapp\nspec:\n  selector:\n    app: myapp\n  ports:\n  - port: &lt;Port&gt;\n    targetPort: &lt;Target Port&gt;\n</code></pre> <ul> <li>If the Pods are distributed across multiple nodes  </li> </ul> <p>To summarize, in any case, whether it be a single Pod on a single node, multiple Pods on a single node, or multiple Pods on multiple nodes, the service is created exactly the same without you having to do any additional steps during the service creation. When Pods are removed or added, the service is automatically updated,</p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.3%20K8s-services/#cluster-ip-service","title":"Cluster IP Service","text":"<p>Creates a virtual IP within the cluster, allowing internal communication between services like frontend and backend.  </p> <ul> <li> <p>Pods in Kubernetes are assigned IP addresses, but these are not static\u2014pods can be recreated at any time, making direct IP-based communication unreliable. For instance, if a frontend pod needs to talk to the backend, how does it know which backend pod to connect to?</p> </li> <li> <p>This is where a Kubernetes Service helps. It groups similar pods (like backend pods) under a single stable IP and DNS name, enabling reliable internal communication. The service automatically load balances requests across the pods in the group.</p> </li> <li> <p>Similarly, you can create a service for Redis so backend pods can communicate with it reliably. This setup supports scalable microservices, where each layer can grow or change independently without breaking communication.Such a service is called a ClusterIP service, and it's the standard way for pods to talk to each other inside the cluster.</p> </li> </ul> ClusterIP Service Configuration (Click to Expand) <pre><code># service-definition.yml\napiVersion: v1\nkind: Service\nmetadata:\n  name: back-end\nspec:\n  type: ClusterIP\n  ports:\n    - targetPort: 80\n      port: 80\n  selector:\n    app: myapp\n    type: back-end\n</code></pre> <ul> <li> <p>The targetPort refers to the port on which the backend pod is running (in this case, 80), while the port is the one exposed by the service itself\u2014also set to 80 here. The selector is used to connect the service to the appropriate set of pods.</p> </li> <li> <p>In fact, cluster IP is the default type so even if you didn't specify it it will automatically assume the type to be cluster IP.</p> </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.3%20K8s-services/#loadbalancer-service","title":"LoadBalancer Service","text":"Load Balancer Servive Configuration (Click to Expand) <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: myapp\nspec:\n  type: LoadBalancer\n  selector:\n    app: myapp\n  ports:\n    - port: 80\n      targetPort: 80\n</code></pre> <ul> <li> <p>To make your application accessible to users, they need a single, consistent URL\u2014like votingapp.com or resultapp.com\u2014instead of dealing with node IPs and ports.</p> </li> <li> <p>If you're using a supported cloud platform such as Google Cloud, AWS, or Azure, you can take advantage of their built-in load balancer. To do this, simply set the service type of your frontend services to LoadBalancer instead of NodePort.</p> </li> <li> <p>Keep in mind, this approach only works on platforms that support this feature\u2014GCP, AWS, and Azure are all supported.</p> </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.3%20K8s-services/#kubernetes-service-types-comparison","title":"\ud83d\udd01 Kubernetes Service Types: Comparison","text":"Service Type Description Access Scope Use Case ClusterIP Default service type. Exposes the service on an internal IP within the cluster. Internal only Internal communication between pods/services. NodePort Exposes the service on a static port on each node\u2019s IP. Internal + External access via <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code> Basic external access (e.g., dev/test environments). LoadBalancer Provisions an external load balancer and exposes the service to the internet. External (cloud platforms only) Production-level public access. <p>\ud83e\udde0 When to Use What?</p> <ul> <li> <p>ClusterIP: Use when services only need to communicate within the cluster (e.g., frontend talks to backend, backend to database).</p> </li> <li> <p>NodePort: Use when you need basic external access to a service (for testing or non-production environments). Requires using <code>&lt;NodeIP&gt;:&lt;NodePort&gt;</code>.</p> </li> <li> <p>LoadBalancer: Use in cloud environments (AWS, GCP, Azure) when you want a public URL/IP for external access. It automatically handles provisioning of a load balancer.</p> </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.4%20K8s-NameSpaces/","title":"1.4 K8s NameSpaces","text":"<p>Kubernetes creates three default namespaces at cluster setup:</p> <ul> <li> <p>default: For general user workloads.</p> </li> <li> <p>kube-system: For internal Kubernetes components (like networking, DNS).</p> </li> <li> <p>kube-public: For resources accessible to all users.</p> </li> <li> <p>While small or learning environments can work within the default namespace, larger or production setups benefit from custom namespaces (e.g., dev, prod) to isolate resources, apply access policies, and manage resource quotas. This helps prevent accidental changes and ensures better organization and control.</p> </li> <li> <p>In Kubernetes, resources within the same namespace can communicate using just their names (e.g., a web pod can reach a DB service by using dbservice). To access services across namespaces, use the full DNS format: servicename.namespace.svc.cluster.local (e.g., dbservice.dev.svc.cluster.local). Kubernetes automatically creates these DNS entries, where cluster.local is the default cluster domain and svc indicates it\u2019s a service.</p> </li> </ul> <p> </p> NameSpace Commands (Click to Expand) Get Pods in a namespace<pre><code>    kubectl get pods --namespace=kube-system\n\n    (OR)\n\n    kubectl get pods --all-namespaces  #getting pods in all namespaces\n</code></pre> Create pods in another namespace<pre><code>    kubectl create -f pod-definition.yml --namespace=dev\n</code></pre> Set Another Namespace As Default  (Click to Expand) <ul> <li>But what if we want to switch to the <code>dev</code> namespace permanently, so that we don't have to specify the <code>--namespace</code> option anymore?</li> </ul> Set Default Namespace for kubectl<pre><code>kubectl config set-context $(kubectl config current-context) --namespace=dev\n</code></pre> ResourceQuota Configuration (Click to Expand) <ul> <li>To limit resources in a namespace, create a resource quota.</li> </ul> <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-quota\n  namespace: dev\nspec:\n  hard:\n    pods: \"10\"\n    requests.cpu: \"4\"\n    requests.memory: 5Gi\n    limits.cpu: \"10\"\n    limits.memory: 10Gi\n</code></pre> <p></p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.4%20K8s-NameSpaces/#pod-definition-with-namespace","title":"Pod Definition with Namespace","text":"Pod Configuration (Click to Expand) <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: myapp-pod\nnamespace: dev\nlabels:\n    app: myapp\n    type: front-end\nspec:\ncontainers:\n    - name: nginx-container\n    image: nginx\n</code></pre> Create Namespace<pre><code>    kubectl create namespace dev (OR)\n    kubectl create -f namespace-dev.yml\n</code></pre>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.4%20K8s-NameSpaces/#namespace-definition","title":"Namespace Definition","text":"Namespace Creation Definition (Click to Expand) <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\nname: dev\n</code></pre>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/","title":"1.5 imperative and declarative approaches","text":"<ul> <li>Note : A more effective approach is to first modify the local copy of the object\u2019s configuration file by updating the image name as needed, and then use the kubectl replace command to apply the changes and update the object.</li> </ul> <ul> <li> <p>The declarative approach involves using the same object configuration files we've been working with. However, instead of using the create or replace commands, we use kubectl apply to manage the objects.</p> </li> <li> <p>The kubectl apply command is smart enough to create an object if it doesn\u2019t already exist. When working with multiple configuration files \u2014 which is common \u2014 you can specify a directory path rather than a single file, allowing all the objects in that directory to be created at once.</p> </li> <li> <p>To make changes, simply update the relevant configuration file and run kubectl apply again. If the object already exists, apply will detect that and only apply the necessary updates.</p> </li> <li> <p>This approach avoids errors about objects already existing or updates being invalid \u2014 kubectl apply automatically figures out the correct way to update the object based on the changes you've made.</p> </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#certification-tips-imperative-commands-with-kubectl","title":"Certification Tips \u2013 Imperative Commands with Kubectl","text":"<p>While you will mostly use the declarative approach with definition files, imperative commands are useful for quickly completing one-time tasks or generating YAML templates. This can save significant time during certification exams.</p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#key-options","title":"\ud83d\udd27 Key Options","text":"<ul> <li> <p><code>--dry-run=client</code>   Prevents the resource from being created. Useful for testing commands without making changes.</p> </li> <li> <p><code>-o yaml</code>   Outputs the object definition in YAML format.</p> </li> </ul> <p>\ud83d\udca1 Tip: Use both options together to generate definition files you can edit and apply later.</p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#pod","title":"\ud83d\udc33 POD","text":""},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#create-an-nginx-pod","title":"Create an NGINX Pod","text":"<pre><code>kubectl run nginx --image=nginx\n</code></pre>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#generate-pod-yaml-manifest-dry-run","title":"Generate POD YAML Manifest (dry-run)","text":"<pre><code>kubectl run nginx --image=nginx --dry-run=client -o yaml\n</code></pre>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#create-a-deployment","title":"Create a Deployment","text":"<pre><code>kubectl create deployment --image=nginx nginx\n</code></pre>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#generate-deployment-yaml-dry-run","title":"Generate Deployment YAML (dry-run)","text":"<pre><code>kubectl create deployment --image=nginx nginx --dry-run=client -o yaml\n</code></pre>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#create-deployment-with-4-replicas","title":"Create Deployment with 4 Replicas","text":"<pre><code>kubectl create deployment nginx --image=nginx --replicas=4\n</code></pre>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#scale-a-deployment","title":"Scale a Deployment","text":"<pre><code>kubectl scale deployment nginx --replicas=4\n</code></pre>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#generate-and-save-deployment-yaml-to-file","title":"Generate and Save Deployment YAML to File","text":"<pre><code>kubectl create deployment nginx --image=nginx --dry-run=client -o yaml &gt; nginx-deployment.yaml\n</code></pre> <p>Then, update the YAML file to add replicas or modify other fields before applying.</p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#service","title":"\ud83c\udf10 Service","text":""},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#create-clusterip-service-for-pod-redis-on-port-6379","title":"Create ClusterIP Service for Pod <code>redis</code> on Port 6379","text":"<pre><code>kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml\n</code></pre> <p>\u2705 Automatically uses the pod's labels as selectors.</p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#alternative-create-clusterip-service-manual-selectors-required","title":"Alternative: Create ClusterIP Service (Manual Selectors Required)","text":"<pre><code>kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml\n</code></pre> <p>\u26a0\ufe0f Does not use pod labels. Assumes <code>app=redis</code> as selector.</p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#create-nodeport-service-for-pod-nginx-exposing-port-80-on-nodeport-30080","title":"Create NodePort Service for Pod <code>nginx</code> Exposing Port 80 on NodePort 30080","text":"<pre><code>kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml\n</code></pre> <p>\u2705 Uses pod's labels as selectors \u26a0\ufe0f You cannot specify the nodePort in this command \u2014 must edit YAML manually</p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#alternative-specify-nodeport-manual-selectors-required","title":"Alternative: Specify NodePort (Manual Selectors Required)","text":"<pre><code>kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml\n</code></pre> <p>\u26a0\ufe0f Does not use pod's labels as selectors</p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#recommendation","title":"\u2705 Recommendation","text":"<p>Use <code>kubectl expose</code> to generate the YAML and manually add the <code>nodePort</code> before applying it.</p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.5%20imperative%20and%20declarative%20approaches/#references","title":"\ud83d\udcda References","text":"<ul> <li>kubectl Command Reference </li> <li>kubectl Conventions</li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.6%20practice-resources/","title":"1.6 practice resources","text":"CKA Exam Domains &amp; Weightage (Click to Expand) <p>1. Cluster Architecture, Installation &amp; Configuration (25%) <pre><code>- Manage role based access control (RBAC)\n- Prepare underlying infrastructure for installing a Kubernetes cluster\n- Create and manage Kubernetes clusters using kubeadm\n- Manage the lifecycle of Kubernetes clusters\n- Implement and configure a highly-available control plane\n- Use Helm and Kustomize to install cluster components\n- Understand extension interfaces (CNI, CSI, CRI, etc.)\n- Understand CRDs, install and configure operators\n</code></pre></p> <p>2. Workloads &amp; Scheduling (15%) <pre><code>- Understand application deployments and how to perform rolling update and rollbacks\n- Use ConfigMaps and Secrets to configure applications\n- Configure workload autoscaling\n- Understand the primitives used to create robust, self-healing, application deployments\n- Configure Pod admission and scheduling (limits, node affinity, etc.)\n</code></pre></p> <p>3. Services &amp; Networking (20%) <pre><code>- Understand connectivity between Pods\n- Define and enforce Network Policies\n- Use ClusterIP, NodePort, LoadBalancer service types and endpoints\n- Use the Gateway API to manage Ingress traffic\n- Know how to use Ingress controllers and Ingress resources\n- Understand and use CoreDNS\n</code></pre></p> <p>4. Storage (10%) <pre><code>- Implement storage classes and dynamic volume provisioning\n- Configure volume types, access modes and reclaim policies\n- Manage persistent volumes and persistent volume claims\n</code></pre></p> <p>5. Troubleshooting (30%) <pre><code>- Troubleshoot clusters and nodes\n- Troubleshoot cluster components\n- Monitor cluster and application resource usage\n- Manage and evaluate container output streams\n- Troubleshoot services and networking\n</code></pre></p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.6%20practice-resources/#cka-exam-dumps","title":"CKA Exam Dumps","text":"CKA Exam Dumps \u2013 itexams.com <p>     Browse sample questions and practice tests for the Certified Kubernetes Administrator (CKA) exam.     Includes realistic exam-like scenarios.   </p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.6%20practice-resources/#kodekloud","title":"KodeKloud","text":"<ul> <li>Free practice labs</li> <li>Solution Videos</li> <li>Community Support</li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.6%20practice-resources/#killercodacom","title":"killercoda.com","text":"<ul> <li>Interactive K8s environment</li> <li>Real-time feedback</li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.6%20practice-resources/#play-with-k8scom","title":"play-with-k8s.com","text":"<ul> <li>4-hour free clusters</li> <li>Multi-node setup</li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/1.Core-Concepts/1.7%20practice-links/","title":"1.7 practice links","text":"<p>Credits: The following practice tests are from KodeKloud Labs and are part of the Kubernetes Udemy course by Mumshad Mannambeth.</p> <ul> <li>Practice Test - Pods </li> <li>Practice Test - ReplicaSets </li> <li>Practice Test - Deployments </li> <li>Practice Test - Services </li> <li>Practice Test - Namespaces </li> <li>Practice Test - Imperative Commands </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.1%20Manual-Scheduling/","title":"2.1 Manual Scheduling","text":""},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.1%20Manual-Scheduling/#working-of-scheduler-in-kubernetes","title":"Working of Scheduler in Kubernetes","text":"<ul> <li>Each pod has a <code>nodeName</code> field, which is empty by default.  </li> <li>You usually don\u2019t set this field in the manifest\u2014Kubernetes sets it automatically.  </li> <li>The scheduler looks for pods without a <code>nodeName</code>.  </li> <li>It runs its algorithm to find a suitable node.  </li> <li>Once identified, it schedules the pod by setting <code>nodeName</code> through a binding object.  </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.1%20Manual-Scheduling/#scheduling-without-a-scheduler","title":"Scheduling Without a Scheduler","text":"<p>If there is no scheduler, pods will remain in the Pending state. You can still assign them manually in two ways:</p> Case How it Works Notes During Pod Creation Set the <code>nodeName</code> field in the pod spec before creating the pod. Kubernetes then assigns the pod to that node. \u2022 Only possible at creation time.\u2022 Simple and direct method. After Pod Creation Create a Binding object and send a POST request to the Pods Binding API, specifying the target node in JSON format. \u2022 <code>nodeName</code> cannot be modified after creation.\u2022 Mimics the scheduler\u2019s binding process.\u2022 Requires YAML \u2192 JSON conversion."},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.1%20Manual-Scheduling/#troubleshooting-pods-in-pending-state","title":"Troubleshooting Pods in Pending State","text":"<p>If a pod is stuck in the Pending state, use the following checklist:</p> Area What to Check Command(s) Pod Events and Description Review events for scheduling errors or resource issues (CPU, memory, storage). <code>kubectl describe pod &lt;pod-name&gt;</code> Node Status Ensure nodes are Ready and have enough free CPU, memory, and disk space. <code>kubectl get nodes</code><code>kubectl describe node &lt;node-name&gt;</code> Scheduler Availability Verify kube-scheduler is running. If it\u2019s down, pods will remain Pending. (Check scheduler pod logs in managed clusters) Resource Requests and Limits Confirm pod <code>resources.requests</code> don\u2019t exceed node capacity. Compare with available resources. (Check pod spec and node resources with <code>kubectl describe</code>) Taints and Tolerations Check if nodes have taints; pods need matching tolerations. <code>kubectl describe node &lt;node-name&gt; \\| grep Taints</code> Node Selectors and Affinity Validate scheduling rules like <code>nodeSelector</code>, <code>nodeAffinity</code>, or <code>podAffinity</code>/<code>podAntiAffinity</code>. (Check pod spec YAML) Persistent Volumes and Claims Ensure PVCs are bound and a matching PV exists. <code>kubectl get pvc</code>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.1%20Manual-Scheduling/#quick-reference","title":"\u2705 Quick Reference","text":"<ol> <li>Check Pod Events \u2192 <code>kubectl describe pod</code>.  </li> <li>Check Nodes \u2192 <code>kubectl get nodes</code> \u2192 <code>kubectl describe node</code>.  </li> <li>Confirm Scheduler is running.  </li> <li>Validate Resource Requests/Limits.  </li> <li>Inspect Taints/Tolerations.  </li> <li>Review Node Selectors / Affinity rules.  </li> <li>Verify PVCs and PVs.  </li> </ol>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.2%20Labels-and-Selectors/","title":"2.2 Labels and Selectors","text":"<ul> <li>For example, here's a manifest for a Pod that has two labels environment: production and app: nginx:</li> </ul> Pod Labels Configuration in YAML (Click to Expand) <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: label-demo\n  labels:\n    environment: production  # Label 1\n    app: nginx               # Label 2\nspec:\n  containers:\n    - name: nginx\n      image: nginx:1.14.2\n      ports:\n        - containerPort: 80\n</code></pre> <p>Select Pods using a Label Selector<pre><code>kubectl get pods --selector app=nginx (or)\nkubectl get pods -l env=prod,bu=finance,tier=frontend #if Pods have multiple labels\n\n\ud83d\udd0e Explanation:\n-l (or --selector) lets you filter by labels.\nComma-separated labels mean AND condition \u2192 Pod must match all of them.\n</code></pre> </p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.2%20Labels-and-Selectors/#annotation","title":"Annotation","text":"<ul> <li> <p>Labels and selectors are primarily used to group and identify Kubernetes objects, whereas annotations serve to store additional information for reference.</p> </li> <li> <p>For instance, annotations can capture details such as tool name, version, build information, and other metadata.</p> </li> </ul> Pod with Annotation in YAML (Click to Expand) <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: annotations-demo\n  annotations:\n    imageregistry: \"https://hub.docker.com/\"  # Annotation\nspec:\n  containers:\n    - name: nginx\n      image: nginx:1.14.2\n      ports:\n        - containerPort: 80\n</code></pre>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.3%20Taints-and-Tolerations/","title":"2.3 Taints and Tolerations","text":"<p>Taint (on Node): says \u201cdon\u2019t accept certain Pods unless they have permission.\u201d Toleration (on Pod): is that permission which allows the Pod to run on a tainted Node.  </p> <p>Key Point</p> <ul> <li>Taints keep Pods away from Nodes.  </li> <li>Tolerations let Pods through, but don\u2019t force them onto that Node.  </li> <li>To force Pods onto specific Nodes, use Node Affinity.  </li> <li>Remember: Taints are set on Nodes and Tolerations are set on Pods.  </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.3%20Taints-and-Tolerations/#taint-effects","title":"Taint Effects","text":"<p>Taints can have 3 different effects:</p> <ul> <li>NoSchedule \u2192 Pod without toleration will not be scheduled on the node.  </li> <li>PreferNoSchedule \u2192 Kubernetes tries not to schedule Pods without toleration, but may still do so if needed.  </li> <li>NoExecute \u2192 Pod without toleration will be evicted if it\u2019s already running, and new Pods without toleration won\u2019t be scheduled.  </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.3%20Taints-and-Tolerations/#example","title":"Example","text":"<p>We now have 3 nodes, each with a different taint effect:</p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.3%20Taints-and-Tolerations/#nodes-their-taints","title":"Nodes &amp; Their Taints","text":"<ul> <li>Node 1 \u2013 NoSchedule \ud83d\udc49 Pod without toleration \u274c won\u2019t be scheduled.  </li> <li>Node 2 \u2013 PreferNoSchedule \ud83d\udc49 Pod \u26a0\ufe0f might still be scheduled if no better option.  </li> <li>Node 3 \u2013 NoExecute \ud83d\udc49 Pod \u26d4 is evicted if already running, and new ones are blocked.  </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.3%20Taints-and-Tolerations/#pods-in-this-scenario","title":"Pods in This Scenario","text":"<ul> <li>Pod 1 \u2192 no toleration  </li> <li>Pod 2 \u2192 no toleration  </li> <li>Pod 3 \u2192 tolerates Node 2\u2019s taint  </li> <li>Pod 4 \u2192 tolerates Node 1\u2019s taint  </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.3%20Taints-and-Tolerations/#taint-syntax","title":"Taint Syntax","text":"Add a Taint to a Node<pre><code>kubectl taint nodes &lt;node-name&gt; &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;\n</code></pre>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.3%20Taints-and-Tolerations/#taint-example","title":"Taint Example","text":"Add a Taint to Node1<pre><code>kubectl taint nodes node1 app=blue:NoSchedule\n</code></pre>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.3%20Taints-and-Tolerations/#tolerations-example","title":"Tolerations example","text":"Tolerations for the pod<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\nspec:\n  containers:\n    - name: nginx-container\n      image: nginx\n  tolerations:\n    - key: \"app\"\n      operator: \"Equal\"\n      value: \"blue\"\n      effect: \"NoSchedule\"\n</code></pre> <p>Note</p> <p>Remember, all of these values inside tolerations need to be encoded in double quotes.</p>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.3%20Taints-and-Tolerations/#master-node-taints","title":"Master Node Taints","text":"<ul> <li> <p>Master nodes can technically run Pods like worker nodes, but by default the scheduler avoids them. This is because a taint is automatically applied to master nodes when the cluster is first set up.  </p> </li> <li> <p>You can view or modify this taint if needed, but best practice is not to run application workloads on master nodes.  </p> </li> </ul> Check Taints on Master Node<pre><code>kubectl describe node &lt;master-node-name&gt; | grep Taint\n</code></pre>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.3%20Taints-and-Tolerations/#remove-a-taint-from-a-node","title":"Remove a Taint from a Node","text":"<ul> <li> <p>To remove a taint from a node in Kubernetes, you append a <code>-</code> at the end of the taint specification.  </p> </li> <li> <p>For example, to remove the NoSchedule taint from the <code>controlplane</code> node, run:  </p> </li> </ul> Remove NoSchedule Taint from controlplane<pre><code>kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-\n</code></pre>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.4%20Node-Selectors/","title":"2.4 Node Selectors","text":"<ul> <li>Cluster has 3 nodes: 2 small (limited resources) and 1 large (higher capacity).  </li> <li>Heavy data-processing jobs should run on the large node.  </li> <li>By default, Kubernetes can schedule pods on any node.  </li> <li>This may place heavy pods on small nodes, which is not desired.  </li> <li>To avoid this, restrict pods to specific nodes using node selectors (simplest method).  </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.4%20Node-Selectors/#nodeselector-example","title":"NodeSelector Example","text":"<p>Important</p> <ul> <li>Nodes are labeled with key-value pairs (e.g., <code>size=large</code>).  </li> <li>The scheduler uses these labels to decide where to place pods.  </li> <li>Nodes must be labeled before using them in a <code>nodeSelector</code>.  </li> </ul> Node Labeling Command<pre><code>kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt;\n# Example: kubectl label nodes node-1 size=Large\n</code></pre> Pod Configuration with NodeSelector (Click to Expand) <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\nspec:\n  containers:\n    - name: data-processor\n      image: data-processor\n  nodeSelector:\n    size: Large\n</code></pre> <ul> <li> <p>In the pod definition file, you can add a <code>nodeSelector</code> under the <code>spec</code> section with <code>size: large</code> to ensure the data-processing pod runs only on the larger node.  </p> </li> <li> <p>This works because nodes are assigned labels (e.g., <code>size=large</code>), and the Kubernetes scheduler matches these labels with the node selector to decide where the pod should run.</p> </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.4%20Node-Selectors/#node-selector-limitations","title":"Node Selector Limitations","text":"<ul> <li>NodeSelector works for simple cases with a single label.  </li> <li>Complex rules (e.g., run on <code>large OR medium</code> nodes, or avoid <code>small</code> nodes) are not possible with NodeSelector.  </li> <li>For such scenarios, use Node Affinity and Anti-Affinity.  </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.5%20Node-Affinity/","title":"2.5 Node Affinity","text":"<ul> <li>Node Affinity provides advanced control over pod placement compared to NodeSelector.  </li> <li>Defined under <code>spec &gt; affinity &gt; nodeAffinity</code>.  </li> <li>Uses matchExpressions with <code>key</code>, <code>operator</code>, and <code>values</code>.  </li> <li>Example: <code>In</code>, <code>NotIn</code>, <code>Exists</code>.  </li> <li>Example rules:  </li> <li>Place pod on <code>Large</code> or <code>Medium</code> nodes.  </li> <li>Avoid scheduling on <code>Small</code> nodes.  </li> <li>Match nodes where label <code>size</code> exists.  </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.5%20Node-Affinity/#node-affinity-types","title":"Node Affinity Types","text":"<ul> <li>Two stages in pod lifecycle:  </li> <li>DuringScheduling \u2013 pod creation.  </li> <li> <p>DuringExecution \u2013 pod already running.  </p> </li> <li> <p>Available Types:  </p> </li> <li><code>requiredDuringSchedulingIgnoredDuringExecution</code> \u2192 Pod scheduled only if matching node exists.  </li> <li> <p><code>preferredDuringSchedulingIgnoredDuringExecution</code> \u2192 Tries matching node, falls back to any node.  </p> </li> <li> <p>Planned Type:  </p> </li> <li><code>requiredDuringSchedulingRequiredDuringExecution</code> \u2192 Will enforce rules even after pod is running.  </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.5%20Node-Affinity/#behavior-summary","title":"Behavior Summary","text":"<ul> <li>Required: Pod won\u2019t be scheduled without a matching node.  </li> <li>Preferred: Scheduler tries to match, but runs pod elsewhere if needed.  </li> <li>Ignored During Execution: Running pods are not affected if labels change later.  </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.6%20Taints-and-Tolerations-vs-Node-Affinity/","title":"2.6 Taints and Tolerations vs Node Affinity","text":""},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.6%20Taints-and-Tolerations-vs-Node-Affinity/#combining-taints-tolerations-and-node-affinity","title":"Combining Taints, Tolerations, and Node Affinity","text":"<ul> <li>Use taints &amp; tolerations \u2192 block other pods from running on specific nodes.  </li> <li>Use node affinity \u2192 ensure your pods run only on the intended nodes.  </li> <li>Together, they dedicate nodes for specific workloads.  </li> </ul>"},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.7%20practice-links/","title":"2.7 practice links","text":""},{"location":"%E2%98%B8%EF%B8%8F%20Kubernetes/2.Scheduling/2.7%20practice-links/#practice-tests","title":"Practice Tests","text":"<p>Credits: The following practice tests are from KodeKloud Labs and are part of the Kubernetes Udemy course by Mumshad Mannambeth.</p> <ul> <li>Practice Test - Manual Scheduling </li> <li>Practice Test - Labels and Selectors </li> <li>Practice Test - Taints and Tolerations </li> <li>Practice Test - Node Affinity </li> </ul>"},{"location":"%E2%9A%99%EF%B8%8F%20CI-CD/","title":"Index","text":"Github-Actions    <p>use unseen to remove the gif background</p> <p>For full documentation visit mkdocs.org.</p>"},{"location":"%E2%9A%99%EF%B8%8F%20CI-CD/#commands","title":"Commands","text":"<p><code>python3 -m venv venv</code> <code>source venv/bin/active</code> - start venv</p> <ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"%E2%9A%99%EF%B8%8F%20CI-CD/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> create_folder.sh<pre><code>mkdir k8s-manifests \n</code></pre> main.py<pre><code>def main(){}\n</code></pre> <p>Note</p> <p>hi</p> <p>Abstract</p> <p>hi</p> <p>Info</p> <p>hi</p> <p>Tip</p> <p>hi</p> <p>Success</p> <p>hi</p> <p>Question</p> <p>hi</p> <p>Warning</p> <p>hi</p> <p>Failure</p> <p>hi</p> <p>Danger</p> <p>hi</p> <p>Bug</p> <p>hi</p> <p>Example</p> <p>hi</p> <p>Quote</p> <p>hi</p>"},{"location":"%F0%9F%8C%90%20Networking/","title":"Index","text":"Networking    <p>use unseen to remove the gif background</p>"},{"location":"%F0%9F%8C%90%20Networking/#commands","title":"Commands","text":"<p><code>python3 -m venv venv</code> <code>source venv/bin/active</code> - start venv</p> <ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"%F0%9F%8C%90%20Networking/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> create_folder.sh<pre><code>mkdir k8s-manifests \n</code></pre> main.py<pre><code>def main(){}\n</code></pre> <p>Note</p> <p>hi</p> <p>Abstract</p> <p>hi</p> <p>Info</p> <p>hi</p> <p>Tip</p> <p>hi</p> <p>Success</p> <p>hi</p> <p>Question</p> <p>hi</p> <p>Warning</p> <p>hi</p> <p>Failure</p> <p>hi</p> <p>Failure</p> <p>hi</p> <p>Danger</p> <p>hi</p> <p>Bug</p> <p>hi</p> <p>Example</p> <p>hi</p> <p>Quote</p> <p>hi</p>"},{"location":"%F0%9F%8F%97%EF%B8%8F%20Terraform/","title":"Index","text":""},{"location":"%F0%9F%8F%97%EF%B8%8F%20Terraform/#commands","title":"Commands","text":"<p><code>python3 -m venv venv</code> <code>source venv/bin/active</code> - start venv</p> <ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"%F0%9F%8F%97%EF%B8%8F%20Terraform/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> create_folder.sh<pre><code>mkdir k8s-manifests \n</code></pre> main.py<pre><code>def main(){}\n</code></pre> <p>Note</p> <p>hi</p> <p>Abstract</p> <p>hi</p> <p>Info</p> <p>hi</p> <p>Tip</p> <p>hi</p> <p>Success</p> <p>hi</p> <p>Question</p> <p>hi</p> <p>Warning</p> <p>hi</p> <p>Failure</p> <p>hi</p> <p>Failure</p> <p>hi</p> <p>Danger</p> <p>hi</p> <p>Bug</p> <p>hi</p> <p>Example</p> <p>hi</p> <p>Quote</p> <p>hi</p>"},{"location":"%F0%9F%90%8D%20Python%2Bscripting/","title":"Index","text":""},{"location":"%F0%9F%90%8D%20Python%2Bscripting/#commands","title":"Commands","text":"<p><code>python3 -m venv venv</code> <code>source venv/bin/active</code> - start venv</p> <ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"%F0%9F%90%8D%20Python%2Bscripting/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> create_folder.sh<pre><code>mkdir k8s-manifests \n</code></pre> main.py<pre><code>def main(){}\n</code></pre> <p>Note</p> <p>hi</p> <p>Abstract</p> <p>hi</p> <p>Info</p> <p>hi</p> <p>Tip</p> <p>hi</p> <p>Success</p> <p>hi</p> <p>Question</p> <p>hi</p> <p>Warning</p> <p>hi</p> <p>Failure</p> <p>hi</p> <p>Failure</p> <p>hi</p> <p>Danger</p> <p>hi</p> <p>Bug</p> <p>hi</p> <p>Example</p> <p>hi</p> <p>Quote</p> <p>hi</p>"},{"location":"%F0%9F%90%A7%20Linux%2BScripting/","title":"Index","text":"Linux"},{"location":"%F0%9F%90%A7%20Linux%2BScripting/#commands","title":"Commands","text":"<p><code>python3 -m venv venv</code> <code>source venv/bin/active</code> - start venv</p> <ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"%F0%9F%90%A7%20Linux%2BScripting/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> create_folder.sh<pre><code>mkdir k8s-manifests \n</code></pre> main.py<pre><code>def main(){}\n</code></pre> <p>Note</p> <p>hi</p> <p>Abstract</p> <p>hi</p> <p>Info</p> <p>hi</p> <p>Tip</p> <p>hi</p> <p>Success</p> <p>hi</p> <p>Question</p> <p>hi</p> <p>Warning</p> <p>hi</p> <p>Failure</p> <p>hi</p> <p>Failure</p> <p>hi</p> <p>Danger</p> <p>hi</p> <p>Bug</p> <p>hi</p> <p>Example</p> <p>hi</p> <p>Quote</p> <p>hi</p>"},{"location":"%F0%9F%90%B3%20Docker/","title":"Index","text":""},{"location":"%F0%9F%90%B3%20Docker/#commands","title":"Commands","text":"<p><code>python3 -m venv venv</code> <code>source venv/bin/active</code> - start venv</p> <ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"%F0%9F%90%B3%20Docker/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> create_folder.sh<pre><code>mkdir k8s-manifests \n</code></pre> main.py<pre><code>def main(){}\n</code></pre> <p>Note</p> <p>hi</p> <p>Abstract</p> <p>hi</p> <p>Info</p> <p>hi</p> <p>Tip</p> <p>hi</p> <p>Success</p> <p>hi</p> <p>Question</p> <p>hi</p> <p>Warning</p> <p>hi</p> <p>Failure</p> <p>hi</p> <p>Failure</p> <p>hi</p> <p>Danger</p> <p>hi</p> <p>Bug</p> <p>hi</p> <p>Example</p> <p>hi</p> <p>Quote</p> <p>hi</p>"},{"location":"%F0%9F%93%8A%20Monitoring%20%26%20logging/","title":"Index","text":"Monitoring and Logging    <p>use unseen to remove the gif background</p>"},{"location":"%F0%9F%93%8A%20Monitoring%20%26%20logging/#commands","title":"Commands","text":"<p><code>python3 -m venv venv</code> <code>source venv/bin/active</code> - start venv</p> <ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"%F0%9F%93%8A%20Monitoring%20%26%20logging/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> create_folder.sh<pre><code>mkdir k8s-manifests \n</code></pre> main.py<pre><code>def main(){}\n</code></pre> <p>Note</p> <p>hi</p> <p>Abstract</p> <p>hi</p> <p>Info</p> <p>hi</p> <p>Tip</p> <p>hi</p> <p>Success</p> <p>hi</p> <p>Question</p> <p>hi</p> <p>Warning</p> <p>hi</p> <p>Failure</p> <p>hi</p> <p>Failure</p> <p>hi</p> <p>Danger</p> <p>hi</p> <p>Bug</p> <p>hi</p> <p>Example</p> <p>hi</p> <p>Quote</p> <p>hi</p>"},{"location":"%F0%9F%94%80%20Git/","title":"Index","text":"<p>use unseen to remove the gif background</p>"},{"location":"%F0%9F%94%80%20Git/#commands","title":"Commands","text":"<p><code>python3 -m venv venv</code> <code>source venv/bin/active</code> <code>mkdocs serve --clean</code> <code>mkdocs serve --dev-addr 127.0.0.1:8085</code> - start venv</p> <ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"%F0%9F%94%80%20Git/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> create_folder.sh<pre><code>mkdir k8s-manifests \n</code></pre> main.py<pre><code>def main(){}\n</code></pre> <p>Note</p> <p>hi</p> <p>Abstract</p> <p>hi</p> <p>Info</p> <p>hi</p> <p>Tip</p> <p>hi</p> <p>Success</p> <p>hi</p> <p>Question</p> <p>hi</p> <p>Warning</p> <p>hi</p> <p>Failure</p> <p>hi</p> <p>Failure</p> <p>hi</p> <p>Danger</p> <p>hi</p> <p>Bug</p> <p>hi</p> <p>Example</p> <p>hi</p> <p>Quote</p> <p>hi</p>"}]}